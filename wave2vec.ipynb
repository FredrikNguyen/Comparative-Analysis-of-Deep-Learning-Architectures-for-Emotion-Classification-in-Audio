{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597509cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define emotion mapping for RAVDESS\n",
    "emotion_map = {\n",
    "    1: 'neutral',\n",
    "    2: 'calm',\n",
    "    3: 'happy',\n",
    "    4: 'sad',\n",
    "    5: 'angry',\n",
    "    6: 'fearful',\n",
    "    7: 'disgust',\n",
    "    8: 'surprised'\n",
    "}\n",
    "\n",
    "# Create dataframe from RAVDESS directory\n",
    "def create_dataframe(dataset_path):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                path = os.path.join(root, file)\n",
    "                # RAVDESS filename format: 03-01-06-01-02-01-12.wav\n",
    "                # Emotion is the 3rd number (06 in this example)\n",
    "                emotion_code = int(file.split('-')[2])\n",
    "                emotion = emotion_map[emotion_code]\n",
    "                data.append({'path': path, 'emotion': emotion, 'label': emotion_code-1})  # -1 to make labels 0-based\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Replace with your RAVDESS dataset path\n",
    "dataset_path = 'path/to/ravdess/audio'\n",
    "df = create_dataframe(dataset_path)\n",
    "\n",
    "# Split data into train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, processor, max_length=16000*4):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length  # 4 seconds at 16kHz\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.df.iloc[idx]['path']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        # Load audio file\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample if necessary (Wav2Vec2 expects 16kHz)\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert stereo to mono if needed\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Normalize waveform\n",
    "        waveform = waveform / torch.max(torch.abs(waveform))\n",
    "        \n",
    "        # Trim or pad audio to max_length\n",
    "        if waveform.shape[1] > self.max_length:\n",
    "            waveform = waveform[:, :self.max_length]\n",
    "        else:\n",
    "            padding = self.max_length - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        # Process with Wav2Vec2 processor\n",
    "        input_values = self.processor(\n",
    "            waveform.squeeze().numpy(), \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_values.squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_values': input_values,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EmotionDataset(train_df, processor)\n",
    "test_dataset = EmotionDataset(test_df, processor)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54731703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification\n",
    "\n",
    "# Load pre-trained Wav2Vec2 model with classification head\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\",\n",
    "    num_labels=len(emotion_map),\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75745906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_values)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_values)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc, val_preds, val_labels = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "        torch.save(model.state_dict(), \"best_wav2vec2_emotion_model.pt\")\n",
    "        print(\"Saved best model!\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(val_labels, val_preds, target_names=list(emotion_map.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_path, model, processor, device, max_length=16000*4):\n",
    "    # Load and preprocess audio\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert stereo to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Normalize\n",
    "    waveform = waveform / torch.max(torch.abs(waveform))\n",
    "    \n",
    "    # Trim or pad\n",
    "    if waveform.shape[1] > max_length:\n",
    "        waveform = waveform[:, :max_length]\n",
    "    else:\n",
    "        padding = max_length - waveform.shape[1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "    \n",
    "    # Process\n",
    "    input_values = processor(\n",
    "        waveform.squeeze().numpy(), \n",
    "        sampling_rate=16000, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values.to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    return emotion_map[predicted_class + 1], probabilities.cpu().numpy()[0]\n",
    "\n",
    "# Example usage\n",
    "audio_path = \"path/to/test/audio.wav\"\n",
    "emotion, probabilities = predict_emotion(audio_path, model, processor, device)\n",
    "print(f\"Predicted emotion: {emotion}\")\n",
    "print(\"Probabilities:\", {e: f\"{p:.4f}\" for e, p in zip(emotion_map.values(), probabilities)})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
